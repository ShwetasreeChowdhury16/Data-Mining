{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from random import randrange\n",
    "\n",
    "fr = open(\"project3_dataset2.txt\");\n",
    "stringArr = [line.strip().split('\\t') for line in fr.readlines()]\n",
    "input_data = np.array(stringArr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['132', '6.20', '6.47', ..., '14.14', '45', '0'],\n",
       "       ['123', '0.05', '4.61', ..., '2.78', '16', '0'],\n",
       "       ['128', '0.50', '3.70', ..., '22.73', '28', '0'],\n",
       "       ...,\n",
       "       ['138', '4.50', '2.85', ..., '24.89', '56', '1'],\n",
       "       ['170', '7.60', '5.50', ..., '6.17', '54', '1'],\n",
       "       ['128', '0.00', '10.58', ..., '14.66', '48', '0']], dtype='<U7')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_mean(numbers):\n",
    "    return sum(numbers) / float(len(numbers))\n",
    "\n",
    "def calculate_standard_deviation(numbers):\n",
    "    avg = calculate_mean(numbers)\n",
    "    variance = sum([pow(x - avg, 2) for x in numbers]) / float(len(numbers) - 1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "def get_map(dataset):\n",
    "    values = [(calculate_mean(attribute), calculate_standard_deviation(attribute)) for attribute in zip(*dataset)]\n",
    "    return values\n",
    "\n",
    "def dictionary(data):\n",
    "    dict = {}\n",
    "    for i in range(len(data)):\n",
    "        vector = data[i]\n",
    "        float_vector = vector[:-1].astype(np.float)\n",
    "        if (vector[-1] not in dict):\n",
    "            dict[vector[-1]] = []\n",
    "        dict[vector[-1]].append(float_vector)\n",
    "    return dict\n",
    "\n",
    "def get_class_value(data):\n",
    "    values = {}\n",
    "    map = dictionary(data)\n",
    "#             print(map)\n",
    "    for classValue, instances in map.items():\n",
    "        values[classValue] = get_map(instances)\n",
    "    return values\n",
    "\n",
    "def get_continuous_val(trainingdata,index):\n",
    "        if len(index) > 0:\n",
    "            for i in range(len(index)):\n",
    "                trainingdata = np.delete(trainingdata,index[i],axis=1)\n",
    "        value = get_class_value(trainingdata)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_probability(testdata,summary,index):\n",
    "    if len(index) > 0:\n",
    "        for i in range(len(index)):\n",
    "            testdata = np.delete(testdata,index[i],axis=1)\n",
    "\n",
    "    testdata = testdata[:,:-1].astype(np.float)\n",
    "    continuous_matrix = []\n",
    "    \n",
    "    def classwiseprobability(input_vec, summary):\n",
    "        probability = {}\n",
    "        for classval, classSummary in summary.items():\n",
    "            probability[classval] = 1\n",
    "            for i in range(len(classSummary)):\n",
    "                mean, standard_deviation = classSummary[i]\n",
    "                x = input_vec[i]\n",
    "                probability[classval] *= calculateProbability(x, mean, standard_deviation)\n",
    "        return probability\n",
    "\n",
    "    def calculateProbability(x, mean, standard_deviation):\n",
    "        exponent = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(standard_deviation, 2))))\n",
    "        return (1 / (math.sqrt(2 * math.pi) * standard_deviation)) * exponent\n",
    "\n",
    "    for i in range(len(testdata)):\n",
    "        getprob = []\n",
    "        input_vec = testdata[i]\n",
    "        prob_map = classwiseprobability(input_vec,summary)\n",
    "        getprob.append(prob_map['0'])\n",
    "        getprob.append(prob_map['1'])\n",
    "        continuous_matrix.append(getprob)\n",
    "\n",
    "    return continuous_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_val(trainingdata,testdata,index):\n",
    "    if len(index) > 0 :\n",
    "        final_catmatrix = 1\n",
    "        for j in range(len(index)):\n",
    "            train_data = trainingdata[:,index[j]]\n",
    "            label_train = trainingdata[:,-1]\n",
    "            dict = {}\n",
    "            for i in range(len(label_train)):\n",
    "                if label_train[i] not in dict:\n",
    "                    dict[label_train[i]] = []\n",
    "                dict[label_train[i]].append(train_data[i])\n",
    "            cat_matrix1,prior = get_categorical_probability(testdata,dict,index[j])\n",
    "            final_catmatrix *= cat_matrix1\n",
    "        final_catmatrix = np.multiply(final_catmatrix,prior)\n",
    "        return final_catmatrix\n",
    "    else: return 1\n",
    "\n",
    "def get_categorical_probability(testdata,summary,index):\n",
    "    label = testdata[:,-1]\n",
    "    testdata = testdata[:,index]\n",
    "    categorical_matrix = []\n",
    "    def get_classwise_probability(summary,input_vector,label):\n",
    "        prob1 = {}\n",
    "        for classval, data1 in summary.items():\n",
    "            prob1[classval] = 1\n",
    "            if classval not in prob1:\n",
    "                prob1[classval] = []\n",
    "            prior = len(summary[classval])/len(label)\n",
    "            posterior = data1.count(input_vector) / len(summary[classval])\n",
    "            prob1[classval] *= posterior\n",
    "        return prob1,prior\n",
    "\n",
    "    for i in range(len(testdata)):\n",
    "        getprob = []\n",
    "        input_vector = testdata[i]\n",
    "        prob,prior = get_classwise_probability(summary,input_vector,label)\n",
    "        getprob.append(prob['0'])\n",
    "        getprob.append(prob['1'])\n",
    "        categorical_matrix.append(getprob)\n",
    "\n",
    "    return categorical_matrix,prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getindex(data):\n",
    "    index = []\n",
    "    for i in range(data.shape[1]):\n",
    "        try:\n",
    "            data[0,i].astype(np.float)\n",
    "        except ValueError:\n",
    "            index.append(i)\n",
    "    return index\n",
    "\n",
    "def getpredictedlabels(x):\n",
    "    predicted_labels = []\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[i])):\n",
    "            if x[i][j] == np.max(x[i]):\n",
    "                predicted_labels.append(j)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivebayes(trainingdata,testingdata):\n",
    "    print(trainingdata)\n",
    "\n",
    "    x = getindex(trainingdata)\n",
    "#     print(x)\n",
    "    value = get_continuous_val(trainingdata, x)\n",
    "#     print(value)\n",
    "    continuous_matrix = get_continuous_probability(testdata, value, x)\n",
    "    print(continuous_matrix)\n",
    "    get_categorical_value = get_categorical_val(trainingdata, testdata, x)\n",
    "    predicted_matrix = continuous_matrix * get_categorical_value\n",
    "    predicted_labels = getpredictedlabels(predicted_matrix)\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "def calculate_performance_metrics(actual_values, predicted_values):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(actual_values)):\n",
    "\n",
    "        if actual_values[i] == 1 and predicted_values[i] == 1:\n",
    "            tp = tp + 1\n",
    "        if actual_values[i] == 1 and predicted_values[i] == 0:\n",
    "            fn = fn + 1\n",
    "        if actual_values[i] == 0 and predicted_values[i] == 1:\n",
    "            fp = fp + 1\n",
    "        if actual_values[i] == 0 and predicted_values[i] == 0:\n",
    "            tn = tn + 1\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    F1 = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    if tp + fp != 0:\n",
    "        precision = tp / float(tp + fp)\n",
    "    if tp + fn != 0:\n",
    "        recall = tp / float(fn + tp)\n",
    "    if tp + tn + fp + fn != 0:\n",
    "        accuracy = (tp + tn) / float(tp + tn + fp + fn)\n",
    "    if precision + recall != 0:\n",
    "        F1 = (2 * precision * recall) / float(precision + recall)\n",
    "\n",
    "    precision = precision * 100\n",
    "    recall = recall * 100\n",
    "    accuracy = accuracy * 100\n",
    "    F1 = F1 * 100\n",
    "\n",
    "    return {'precision': precision, 'recall': recall, 'accuracy': accuracy, 'F1': F1}\n",
    "\n",
    "def convert_labels(actual_labels):\n",
    "    final_actual_labels = []\n",
    "    for i in range(len(actual_labels)):\n",
    "        final_actual_labels.append(int(actual_labels[i]))\n",
    "\n",
    "    return final_actual_labels\n",
    "\n",
    "def naive_bayes_demo(dataset,testdata):\n",
    "    print(testdata)\n",
    "    def getindex(dataset):\n",
    "        index = []\n",
    "        for i in range(dataset.shape[1]):\n",
    "            try:\n",
    "                dataset[0,i].astype(np.float)\n",
    "            except ValueError:\n",
    "                index.append(i)\n",
    "        return index\n",
    "\n",
    "    traindata = dataset\n",
    "    index = getindex(dataset)\n",
    "    print(index)\n",
    "    labels = list(dataset[:,-1])\n",
    "\n",
    "    prior_0 = labels.count(\"0\")/len(labels)\n",
    "    prior_1 = labels.count(\"1\")/len(labels)\n",
    "\n",
    "\n",
    "    denominator = 1\n",
    "    for i in range(len(testdata)):\n",
    "        val = list(dataset[:,i])\n",
    "        print(testdata[i])\n",
    "        print(val)\n",
    "        num = val.count(testdata[i])\n",
    "        print(num)\n",
    "        den = num/len(dataset)\n",
    "        denominator = denominator*den\n",
    "    post_0 = 1\n",
    "    post_1 = 1\n",
    "    for i in range(len(index)):\n",
    "        train_data = traindata[:,index[i]]\n",
    "        label_train = traindata[:, -1]\n",
    "        dict = {}\n",
    "        for j in range(len(label_train)):\n",
    "            if label_train[j] not in dict:\n",
    "                dict[label_train[j]] = []\n",
    "            dict[label_train[j]].append(train_data[j])\n",
    "        posterior_0 = dict['0'].count(testdata[i])/len(dict['0'])\n",
    "        posterior_1 = dict['1'].count(testdata[i]) / len(dict['1'])\n",
    "        post_0 = post_0 * posterior_0\n",
    "        post_1 = post_1 * posterior_1\n",
    "    print(prior_0)\n",
    "    print(prior_1)\n",
    "    print(post_0)\n",
    "    print(post_1)\n",
    "    prob_list = []\n",
    "    prob_0 = (post_0*prior_0)/denominator\n",
    "    prob_1 = (post_1*prior_1)/denominator\n",
    "    prob_list.append(prob_0)\n",
    "    prob_list.append(prob_1)\n",
    "    val = prob_list.index(max(prob_list))\n",
    "\n",
    "#     print(\"probability for class 0 : \", prob_0)\n",
    "#     print(\"probability for class 1 : \", prob_1)\n",
    "    print(\"X : \", testdata)\n",
    "    print(\"p(H0|X) : \", prob_0)\n",
    "    print(\"p(H1|X) : \", prob_1)\n",
    "    print(\"The input will get classified to class : \", val)\n",
    "\n",
    "    return prob_0,prob_1,val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sunny', 'cool', 'high', 'weak']\n",
      "[4]\n",
      "sunny\n",
      "['132', '123', '128', '114', '150', '136', '144', '134', '126', '164', '178', '136', '128', '146', '112', '174', '162', '216', '142', '160', '134', '138', '178', '118', '128', '176', '170', '146', '132', '150', '156', '138', '168', '168', '112', '110', '132', '136', '128', '108', '134', '130', '128', '130', '126', '114', '136', '148', '118', '116', '140', '138', '158', '140', '152', '136', '128', '128', '132', '162', '118', '132', '134', '124', '153', '122', '130', '134', '154', '154', '117', '124', '134', '148', '158', '154', '164', '114', '130', '134', '164', '142', '134', '136', '130', '136', '174', '132', '132', '132', '142', '146', '118', '128', '127', '158', '148', '128', '148', '120', '138', '116', '150', '134', '123', '160', '138', '121', '158', '168', '138', '114', '130', '156', '126', '128', '134', '124', '190', '124', '128', '130', '132', '126', '128', '160', '166', '124', '156', '130', '124', '200', '148', '120', '176', '124', '134', '120', '124', '114', '137', '118', '148', '118', '170', '128', '130', '124', '126', '158', '134', '146', '138', '120', '206', '108', '194', '174', '130', '146', '164', '162', '128', '126', '132', '132', '132', '122', '150', '150', '130', '156', '140', '128', '112', '140', '164', '126', '144', '112', '128', '142', '109', '150', '134', '132', '110', '160', '136', '146', '132', '136', '206', '154', '118', '108', '152', '114', '116', '132', '148', '132', '130', '132', '108', '130', '138', '136', '152', '144', '166', '214', '144', '124', '130', '118', '142', '138', '160', '132', '136', '170', '138', '120', '134', '124', '154', '128', '144', '182', '174', '140', '132', '130', '161', '134', '142', '174', '160', '132', '118', '126', '134', '146', '126', '132', '142', '142', '154', '144', '166', '162', '142', '142', '176', '126', '180', '130', '122', '160', '188', '102', '136', '152', '134', '118', '136', '114', '108', '152', '138', '134', '134', '120', '166', '130', '126', '112', '140', '136', '145', '154', '134', '140', '110', '118', '134', '122', '122', '126', '108', '218', '106', '162', '166', '154', '120', '124', '140', '176', '142', '136', '128', '208', '138', '116', '138', '118', '128', '140', '126', '114', '128', '144', '114', '126', '106', '143', '124', '118', '122', '120', '122', '124', '122', '190', '166', '136', '154', '101', '144', '136', '134', '122', '114', '128', '152', '112', '120', '208', '136', '134', '160', '148', '140', '170', '126', '166', '146', '106', '134', '128', '136', '120', '148', '114', '110', '124', '198', '178', '120', '122', '118', '116', '162', '128', '118', '124', '146', '129', '122', '132', '162', '208', '134', '148', '118', '158', '126', '118', '116', '124', '136', '124', '160', '120', '126', '194', '138', '118', '180', '144', '130', '136', '128', '122', '122', '148', '140', '120', '130', '138', '136', '134', '152', '124', '122', '134', '120', '136', '162', '118', '136', '108', '136', '146', '126', '148', '138', '170', '143', '142', '132', '124', '142', '138', '116', '136', '122', '103', '134', '126', '118', '154', '112', '136', '142', '140', '144', '162', '132', '118', '122', '166', '180', '146', '174', '136', '150', '158', '134', '126', '124', '116', '122', '130', '152', '114', '136', '138', '170', '128']\n",
      "0\n",
      "cool\n",
      "['6.20', '0.05', '0.50', '9.60', '0.30', '8.80', '0.76', '11.79', '8.75', '5.60', '20.00', '3.99', '0.00', '0.64', '4.46', '2.02', '0.00', '0.92', '1.32', '12.00', '0.00', '2.00', '0.95', '0.12', '5.16', '5.76', '4.20', '6.40', '0.00', '3.50', '0.00', '8.80', '9.00', '4.50', '0.00', '2.35', '6.00', '7.36', '1.60', '15.00', '3.00', '1.75', '2.60', '2.78', '0.96', '3.60', '6.60', '5.50', '0.00', '31.20', '3.90', '0.00', '16.00', '8.00', '19.45', '5.80', '5.40', '0.00', '7.20', '6.94', '0.00', '2.80', '12.50', '14.00', '7.80', '3.20', '0.54', '2.50', '4.20', '4.50', '1.53', '1.60', '0.00', '0.00', '4.00', '5.53', '8.20', '1.20', '0.05', '2.00', '13.02', '4.05', '0.00', '8.00', '0.00', '0.00', '0.00', '7.90', '8.40', '0.00', '0.00', '10.50', '1.05', '0.70', '0.00', '1.02', '4.80', '0.00', '4.50', '0.00', '0.00', '1.03', '0.18', '6.40', '8.60', '3.00', '0.00', '0.80', '13.50', '11.40', '2.27', '3.00', '1.22', '4.00', '5.10', '0.00', '0.02', '4.82', '4.18', '0.00', '6.60', '0.08', '7.28', '1.80', '2.00', '1.52', '6.00', '6.00', '3.02', '0.00', '15.50', '19.20', '0.00', '0.00', '1.20', '4.00', '3.00', '0.00', '3.04', '0.00', '1.20', '4.00', '0.00', '0.12', '7.50', '0.00', '0.00', '4.00', '8.75', '6.17', '4.80', '4.36', '12.00', '0.00', '6.00', '1.50', '2.55', '9.45', '0.00', '0.00', '0.50', '2.92', '0.04', '0.00', '0.00', '12.30', '2.00', '0.72', '14.40', '13.80', '1.72', '3.00', '0.00', '0.00', '1.44', '5.20', '12.00', '0.54', '4.09', '0.60', '6.00', '3.00', '1.20', '20.00', '0.90', '0.72', '0.00', '4.20', '1.20', '1.16', '2.00', '3.46', '0.00', '2.40', '0.28', '3.00', '12.18', '0.00', '4.28', '0.00', '15.00', '0.10', '2.61', '0.00', '0.00', '18.00', '0.00', '1.50', '3.00', '4.00', '3.00', '0.40', '0.01', '0.00', '0.00', '0.00', '0.00', '0.06', '14.00', '0.00', '2.20', '0.00', '0.87', '3.70', '1.10', '0.00', '0.00', '2.80', '0.04', '4.20', '0.00', '0.45', '0.00', '5.60', '9.00', '10.00', '0.00', '3.50', '1.15', '7.00', '1.62', '0.00', '6.10', '7.50', '0.00', '9.90', '7.44', '0.28', '1.80', '6.75', '0.07', '6.30', '2.40', '0.00', '0.00', '0.09', '0.52', '7.28', '1.00', '7.77', '0.00', '0.40', '1.81', '5.99', '13.60', '1.50', '1.36', '0.00', '0.40', '10.10', '0.60', '14.10', '1.50', '7.50', '0.00', '0.56', '5.50', '4.20', '0.60', '0.00', '9.10', '1.40', '2.75', '8.60', '12.16', '0.00', '12.00', '0.00', '0.00', '3.10', '0.00', '11.20', '1.61', '5.60', '6.00', '0.70', '0.00', '4.25', '0.00', '6.00', '3.72', '11.20', '2.24', '27.40', '1.15', '2.38', '0.00', '1.00', '0.42', '1.68', '3.40', '0.00', '0.40', '0.40', '0.00', '0.00', '1.08', '5.04', '4.20', '1.25', '0.00', '0.00', '0.28', '0.00', '0.00', '5.15', '0.80', '3.15', '0.31', '0.48', '0.00', '2.52', '8.08', '4.18', '0.10', '4.65', '1.68', '0.41', '1.40', '5.04', '3.96', '0.57', '0.00', '6.00', '4.20', '0.40', '0.21', '4.10', '3.60', '5.60', '0.00', '0.73', '8.80', '0.00', '8.20', '0.00', '4.64', '1.80', '0.52', '5.50', '5.50', '4.00', '4.46', '2.70', '7.40', '0.00', '5.40', '0.16', '1.35', '2.15', '1.70', '12.00', '5.30', '7.40', '0.00', '12.20', '0.75', '2.60', '4.60', '0.00', '1.91', '0.00', '0.00', '0.40', '0.60', '10.50', '19.60', '1.70', '6.00', '0.08', '25.01', '0.00', '0.00', '0.00', '0.00', '0.00', '4.40', '0.50', '4.50', '0.00', '4.50', '0.00', '13.20', '0.12', '0.90', '1.04', '4.26', '8.80', '0.00', '5.00', '1.50', '6.00', '7.50', '0.80', '1.70', '0.00', '0.90', '4.04', '0.05', '2.60', '0.46', '4.48', '0.00', '0.81', '18.20', '2.16', '3.00', '0.00', '0.00', '0.03', '0.05', '10.50', '0.12', '4.50', '9.65', '6.80', '2.20', '8.14', '2.40', '7.00', '4.05', '0.00', '6.60', '0.60', '3.57', '5.08', '0.00', '0.40', '0.00', '3.60', '6.00', '3.80', '0.61', '8.00', '3.20', '4.00', '0.00', '4.08', '2.80', '4.50', '7.60', '0.00']\n",
      "0\n",
      "high\n",
      "['6.47', '4.61', '3.70', '2.51', '6.38', '4.69', '10.53', '4.01', '6.53', '3.17', '9.78', '2.58', '2.63', '4.82', '7.18', '6.57', '5.09', '2.66', '7.63', '5.73', '5.63', '5.11', '4.75', '1.96', '4.90', '4.89', '4.67', '5.62', '1.87', '6.99', '3.47', '3.12', '8.53', '6.68', '1.71', '3.36', '5.97', '2.19', '5.41', '4.91', '3.17', '5.46', '4.94', '4.89', '4.99', '4.16', '6.08', '7.10', '3.89', '3.17', '7.32', '2.68', '5.56', '4.42', '4.22', '5.90', '2.36', '8.41', '3.65', '4.55', '1.88', '4.79', '2.73', '6.23', '3.96', '3.59', '3.63', '3.66', '5.59', '4.68', '2.44', '7.22', '3.69', '3.84', '4.18', '3.20', '14.16', '3.98', '2.44', '3.66', '6.26', '3.38', '5.90', '7.85', '1.82', '1.77', '8.46', '2.85', '3.57', '6.63', '3.54', '8.29', '3.16', '4.90', '2.81', '6.33', '6.09', '3.22', '10.49', '3.68', '1.96', '2.83', '4.14', '8.49', '11.17', '9.19', '1.86', '5.29', '5.04', '5.08', '6.41', '7.04', '3.30', '2.05', '2.96', '2.51', '2.80', '3.24', '5.05', '2.28', '3.58', '5.59', '3.52', '6.22', '6.13', '8.12', '3.02', '5.21', '5.35', '2.82', '5.05', '4.43', '5.32', '3.57', '8.28', '6.65', '4.37', '1.07', '4.80', '1.94', '3.14', '3.95', '4.66', '3.26', '6.41', '2.43', '1.88', '12.42', '6.06', '8.12', '6.58', '4.31', '5.13', '5.01', '2.95', '4.33', '6.89', '5.13', '4.16', '4.92', '6.95', '3.63', '8.22', '3.57', '5.80', '5.96', '2.70', '4.04', '5.04', '5.10', '2.66', '1.82', '2.40', '6.34', '2.71', '3.58', '3.91', '4.39', '5.55', '5.28', '4.37', '3.69', '6.14', '6.40', '3.18', '4.37', '7.14', '6.76', '2.78', '2.28', '3.08', '6.38', '4.17', '5.63', '5.80', '1.59', '4.04', '4.97', '7.02', '3.30', '4.98', '3.28', '2.72', '3.55', '1.43', '4.13', '3.24', '6.06', '4.64', '5.03', '3.82', '5.98', '4.41', '4.79', '4.89', '2.99', '4.19', '4.15', '5.90', '0.98', '4.16', '3.12', '1.87', '4.02', '3.54', '3.95', '4.81', '5.53', '3.38', '4.41', '3.27', '4.30', '4.82', '3.37', '4.65', '3.79', '3.72', '5.26', '10.19', '3.20', '9.01', '5.98', '4.77', '7.21', '4.55', '4.63', '5.52', '1.80', '7.13', '5.45', '4.03', '5.73', '2.55', '4.32', '3.14', '5.03', '4.23', '3.56', '5.88', '8.07', '5.47', '3.41', '3.31', '7.99', '3.50', '5.38', '3.16', '2.63', '5.91', '4.71', '3.81', '4.44', '3.73', '15.33', '4.31', '3.30', '3.78', '3.58', '5.56', '5.00', '5.24', '1.72', '5.51', '3.90', '4.99', '3.67', '4.96', '5.49', '3.08', '2.01', '2.74', '2.77', '1.74', '4.24', '8.80', '5.91', '2.77', '8.22', '5.08', '3.98', '4.24', '5.81', '2.83', '3.12', '5.09', '5.67', '3.14', '5.76', '4.60', '11.41', '4.87', '8.01', '6.17', '4.64', '2.99', '5.29', '4.37', '4.86', '2.94', '4.69', '3.37', '3.98', '4.19', '3.23', '5.75', '6.03', '5.63', '4.37', '2.33', '7.26', '3.84', '3.95', '1.55', '9.05', '3.95', '3.31', '3.58', '1.88', '6.25', '5.19', '2.76', '4.75', '2.42', '6.49', '2.91', '4.11', '3.95', '4.00', '3.51', '3.20', '2.43', '3.97', '4.26', '2.42', '7.75', '3.83', '4.55', '3.74', '11.89', '3.79', '3.51', '5.24', '7.27', '3.69', '8.55', '3.09', '11.61', '2.44', '6.39', '5.17', '5.28', '4.51', '7.95', '7.41', '2.40', '3.79', '2.58', '7.46', '7.40', '2.39', '7.56', '3.04', '2.28', '3.67', '6.94', '2.70', '6.03', '6.32', '7.24', '3.48', '3.70', '4.17', '3.92', '4.00', '2.98', '3.76', '3.18', '5.97', '4.59', '2.46', '5.86', '3.96', '7.18', '3.40', '9.12', '2.84', '4.44', '7.41', '3.10', '4.19', '2.46', '9.65', '7.39', '2.47', '3.53', '6.62', '5.64', '3.99', '2.79', '7.22', '2.40', '3.57', '4.17', '6.16', '4.34', '4.90', '3.05', '4.12', '3.05', '4.21', '8.03', '4.49', '4.16', '4.75', '2.29', '7.84', '3.29', '4.93', '8.13', '7.67', '5.15', '4.34', '5.58', '2.42', '3.57', '7.03', '3.86', '3.91', '4.99', '2.97', '3.30', '3.88', '2.69', '6.73', '11.32', '2.40', '6.06', '4.59', '2.53', '2.85', '5.50', '10.58']\n",
      "0\n",
      "weak\n",
      "['36.21', '13.69', '12.81', '29.18', '33.99', '36.07', '35.66', '26.57', '34.02', '30.98', '33.55', '16.38', '23.88', '28.02', '26.25', '31.90', '24.60', '19.85', '29.98', '23.11', '29.12', '31.40', '21.06', '20.31', '31.35', '26.10', '35.45', '33.05', '17.21', '25.39', '21.10', '22.41', '24.48', '28.47', '15.96', '26.72', '25.73', '28.11', '29.30', '34.65', '17.91', '34.34', '21.36', '9.39', '29.74', '22.58', '32.74', '25.31', '15.96', '14.99', '25.05', '17.04', '29.35', '33.15', '29.81', '27.55', '12.98', '28.82', '17.16', '33.36', '10.05', '20.47', '39.35', '35.96', '25.73', '22.49', '22.03', '30.90', '25.02', '39.97', '28.95', '39.68', '13.92', '17.26', '28.61', '28.81', '36.85', '14.90', '28.25', '14.69', '29.38', '16.20', '30.84', '23.81', '10.45', '20.37', '35.10', '26.50', '13.68', '29.58', '16.64', '35.36', '12.98', '37.42', '15.70', '23.88', '36.55', '26.55', '33.27', '12.24', '11.82', '10.85', '14.40', '37.25', '35.28', '26.47', '18.35', '18.95', '30.79', '26.66', '29.07', '22.64', '13.65', '19.48', '26.50', '29.35', '18.84', '21.10', '24.83', '24.86', '20.71', '25.42', '12.33', '19.71', '21.31', '29.30', '29.30', '33.02', '25.72', '19.63', '24.06', '40.60', '26.71', '23.22', '36.16', '30.84', '23.07', '16.02', '19.52', '11.02', '23.87', '18.96', '24.39', '12.26', '38.03', '13.15', '12.51', '31.29', '32.72', '30.75', '29.89', '18.44', '28.34', '26.13', '32.27', '24.99', '33.88', '35.54', '39.43', '18.53', '39.64', '31.33', '28.17', '26.01', '30.96', '32.79', '21.57', '32.38', '26.52', '29.45', '10.38', '27.55', '27.89', '11.87', '22.92', '29.26', '19.59', '21.13', '31.40', '25.71', '22.98', '25.10', '29.26', '35.04', '23.66', '19.54', '28.28', '37.99', '7.12', '34.53', '35.39', '32.25', '33.23', '42.17', '33.70', '15.23', '37.83', '9.69', '19.99', '21.61', '36.94', '10.73', '22.99', '8.66', '26.26', '27.43', '27.68', '26.54', '31.29', '25.78', '26.75', '31.72', '28.61', '34.71', '25.98', '16.17', '18.04', '20.66', '37.12', '21.39', '38.02', '37.15', '15.89', '39.66', '20.41', '36.35', '28.11', '14.29', '23.61', '32.10', '35.40', '24.33', '33.41', '24.80', '15.16', '34.72', '25.68', '21.97', '39.71', '23.26', '21.70', '29.06', '26.08', '25.93', '29.18', '27.86', '33.97', '21.03', '34.04', '29.81', '29.29', '22.61', '23.89', '25.22', '31.04', '13.27', '16.38', '23.29', '34.81', '34.80', '32.44', '17.22', '6.74', '32.48', '27.78', '25.84', '14.97', '9.69', '22.92', '24.65', '28.66', '22.39', '21.53', '22.00', '34.27', '30.86', '34.15', '27.14', '33.39', '27.58', '27.55', '18.86', '26.17', '32.16', '28.56', '12.13', '29.79', '19.56', '16.30', '32.97', '11.17', '30.79', '12.32', '22.53', '37.89', '25.00', '13.35', '30.77', '27.33', '17.20', '32.57', '31.85', '26.48', '26.63', '27.87', '29.01', '12.00', '22.10', '26.68', '29.54', '15.16', '21.64', '26.35', '30.09', '9.74', '27.64', '26.08', '23.59', '27.59', '31.58', '16.10', '13.19', '19.97', '9.64', '30.90', '36.59', '36.21', '20.22', '16.48', '13.00', '18.72', '25.63', '17.50', '29.27', '15.89', '22.74', '25.43', '10.29', '20.47', '20.71', '30.28', '23.07', '34.46', '26.47', '28.83', '42.06', '15.11', '34.30', '22.67', '12.30', '22.24', '23.52', '32.03', '16.66', '34.46', '19.40', '30.46', '16.64', '27.68', '23.92', '23.23', '27.89', '29.13', '13.52', '24.65', '20.57', '30.79', '16.67', '34.21', '27.57', '32.23', '21.93', '33.58', '32.03', '21.11', '34.15', '20.25', '34.07', '31.99', '12.13', '26.45', '17.33', '18.14', '25.76', '30.53', '29.87', '34.99', '33.67', '37.05', '32.28', '38.11', '29.63', '25.55', '19.06', '12.59', '24.59', '11.59', '32.88', '18.01', '13.39', '37.43', '24.70', '35.95', '21.18', '30.23', '16.42', '13.04', '26.84', '26.97', '23.99', '19.39', '33.91', '28.04', '17.53', '20.13', '25.69', '17.78', '20.69', '10.35', '28.69', '22.87', '19.75', '36.57', '11.61', '24.38', '24.83', '30.31', '17.42', '23.51', '18.96', '27.95', '17.33', '9.37', '23.52', '17.20', '30.74', '22.70', '42.49', '35.61', '34.34', '26.51', '30.12', '35.95', '34.03', '36.10', '27.41', '21.73', '21.10', '27.73', '30.11', '28.45', '31.79', '17.15', '28.81', '35.36', '17.42', '41.05', '14.60', '9.28', '30.11', '37.83', '31.81']\n",
      "0\n",
      "0.6536796536796536\n",
      "0.3463203463203463\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-250-323802cd670a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtestdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'sunny'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cool'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'high'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'weak'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnaive_bayes_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-249-a5b79317820a>\u001b[0m in \u001b[0;36mnaive_bayes_demo\u001b[1;34m(dataset, testdata)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mprob_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[0mprob_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpost_0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mprior_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[0mprob_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpost_1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mprior_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[0mprob_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "testdata = ['sunny', 'cool', 'high','weak']\n",
    "naive_bayes_demo(input_data, testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['136' '6.60' '6.08' ... '2.72' '49' '0']\n",
      " ['148' '5.50' '7.10' ... '3.60' '48' '0']\n",
      " ['118' '0.00' '3.89' ... '0.00' '16' '0']\n",
      " ...\n",
      " ['138' '4.50' '2.85' ... '24.89' '56' '1']\n",
      " ['170' '7.60' '5.50' ... '6.17' '54' '1']\n",
      " ['128' '0.00' '10.58' ... '14.66' '48' '0']]\n",
      "[[1.1962439385947202e-12, 5.0403008627415286e-12], [2.904208118817051e-12, 4.236175476903895e-15], [1.9015022974445637e-12, 3.092055500595684e-14], [8.182930912548609e-13, 1.9664870690949102e-12], [2.5341173658184626e-12, 5.5511494494444165e-12], [2.541226192474397e-13, 1.0428269967296292e-12], [1.6379157296078963e-15, 9.269415724104541e-14], [7.056931590249152e-14, 5.982647621368937e-13], [9.377177986027666e-13, 3.553185318057814e-12], [1.1407808902606553e-12, 2.9951350845346893e-12], [1.372497435486478e-20, 3.496168521416386e-15], [1.1786183825055063e-11, 6.533655892826011e-13], [4.877507641999702e-12, 1.1711237288062618e-12], [1.8147873876671006e-11, 7.506607670515785e-12], [8.03583728758018e-13, 5.042767306525589e-13], [2.79524634754561e-13, 2.150659435945319e-12], [1.8186953392380707e-12, 5.4920429506907395e-14], [1.2477485056582887e-16, 3.9687417712166416e-15], [8.638353862527514e-14, 1.4853022017242531e-13], [3.622736783105822e-16, 2.948547625796558e-14], [1.366505071990468e-12, 7.357875289891702e-13], [5.281120980691446e-12, 5.070453850271865e-12], [6.856061762631775e-13, 1.3367092021066762e-12], [3.4652983753218113e-13, 9.038393961294343e-16], [3.684494226801356e-12, 5.462939378848649e-12], [9.238095520551343e-13, 4.800602795262323e-12], [8.045404111447347e-13, 3.66153772534535e-12], [2.7856230893654863e-12, 8.760840037797958e-12], [2.1588221298854162e-12, 2.5984050373253814e-15], [3.425911133107301e-12, 6.510183905687113e-12], [9.847989319210729e-13, 3.2995757652573727e-13], [3.297242271242367e-17, 5.726741070026131e-16], [1.973365164699871e-14, 1.2189392567878987e-12], [9.773265919145073e-13, 4.507913711110438e-12], [3.8689756490798835e-13, 4.776838696237053e-16], [7.65301992464689e-16, 2.7772067669668557e-15], [2.497077161687779e-19, 6.22231154009057e-18], [4.0892360269757113e-13, 1.1336535481267697e-12], [4.188593695126605e-12, 1.3442836922519393e-12], [3.4820421369921725e-15, 4.424948300795526e-13], [4.36575281085463e-12, 6.573808032655875e-14], [3.852684780470943e-12, 5.575428214950025e-12], [1.0620216628963589e-11, 6.937815986129429e-13], [8.44643352775979e-13, 7.014094304910962e-15], [2.699922347031567e-13, 3.3398969124015126e-13], [1.0879052960873558e-12, 1.6339941553439644e-13]]\n",
      "\n",
      " ***  Fold 1 Performance:***************\n",
      "Accuracy: 67.3913043478261 Precision: 60.71428571428571 Recall: 80.95238095238095 F1-Score: 69.38775510204083 \n",
      "\n",
      "[['132' '6.20' '6.47' ... '14.14' '45' '0']\n",
      " ['123' '0.05' '4.61' ... '2.78' '16' '0']\n",
      " ['128' '0.50' '3.70' ... '22.73' '28' '0']\n",
      " ...\n",
      " ['138' '4.50' '2.85' ... '24.89' '56' '1']\n",
      " ['170' '7.60' '5.50' ... '6.17' '54' '1']\n",
      " ['128' '0.00' '10.58' ... '14.66' '48' '0']]\n",
      "[[5.153421184061433e-13, 3.277576137857515e-12], [3.668083022288688e-12, 1.2043935946488224e-11], [5.62134183805353e-13, 1.8080731827677587e-15], [2.461223742050522e-27, 2.0900074702228582e-19], [6.074088684723967e-12, 2.5189509370583575e-12], [1.5369152741104232e-12, 2.933351348411482e-15], [1.777725632396577e-16, 8.708947212973512e-14], [1.3614746955434213e-13, 8.262979881118888e-13], [2.8192045430676525e-18, 4.492321001640226e-15], [3.2517771416503784e-12, 1.0353936797934515e-11], [3.749180398810704e-13, 6.322367375909786e-14], [6.49756917748263e-13, 2.330986618598503e-12], [6.957081045023431e-12, 9.51497932947469e-13], [2.6645926645610146e-12, 9.801413761136854e-12], [2.609618031483484e-13, 2.921533850573538e-16], [2.1211784840440337e-11, 5.988060467369501e-12], [2.920719018552182e-15, 7.979972262718936e-14], [5.113307476300177e-15, 5.110337541579323e-13], [7.372745836685279e-12, 1.5302286128009217e-11], [7.041253330651163e-12, 3.1287293687724938e-12], [7.211045603566642e-12, 1.403524724330299e-12], [2.0672012711865496e-11, 1.4128789098529432e-11], [1.3271071620586971e-11, 1.3635472320960343e-11], [7.630147095384489e-14, 4.885599428473351e-13], [1.9716580736881915e-12, 4.897800928603018e-13], [8.239768775525013e-14, 1.660664105702995e-13], [2.7350470445894775e-12, 8.630534450268011e-15], [5.215783193956939e-13, 7.91001042275233e-15], [2.662079967266254e-12, 3.8782302397962246e-12], [4.6439588303766955e-12, 6.65299340750793e-12], [7.523507307375563e-19, 1.239212510378608e-15], [5.9701275023243144e-12, 7.519418516600874e-14], [1.521312940330162e-12, 4.0815270140896366e-13], [8.967227602439851e-12, 4.0793347944011125e-13], [2.1379070191705948e-14, 2.075408952230779e-12], [6.736625919241742e-12, 7.023868578471894e-13], [5.271756916107885e-12, 6.738696061756398e-12], [1.2323277164403923e-12, 5.137783318126907e-12], [5.421588678679507e-13, 6.242685634405309e-16], [1.4630564117245876e-12, 3.705912364398781e-15], [7.60967499948608e-15, 8.792559043213656e-14], [8.330418253758517e-12, 8.247873961627884e-12], [2.1809275920034924e-13, 9.662391231135353e-14], [6.456003675177847e-13, 8.301050033190316e-13], [1.1602272853176043e-11, 2.7694680736825647e-13], [5.421971653139685e-16, 8.085559760213428e-14]]\n",
      "\n",
      " ***  Fold 2 Performance:***************\n",
      "Accuracy: 67.3913043478261 Precision: 43.47826086956522 Recall: 83.33333333333334 F1-Score: 57.14285714285714 \n",
      "\n",
      "[['132' '6.20' '6.47' ... '14.14' '45' '0']\n",
      " ['123' '0.05' '4.61' ... '2.78' '16' '0']\n",
      " ['128' '0.50' '3.70' ... '22.73' '28' '0']\n",
      " ...\n",
      " ['138' '4.50' '2.85' ... '24.89' '56' '1']\n",
      " ['170' '7.60' '5.50' ... '6.17' '54' '1']\n",
      " ['128' '0.00' '10.58' ... '14.66' '48' '0']]\n",
      "[[3.3319461482875398e-12, 5.709076391571881e-14], [4.09475883746637e-14, 1.0623262272736247e-13], [1.1668463724269662e-12, 2.833208579728564e-15], [2.3925993753834226e-12, 3.0459314528688528e-12], [1.499230490855116e-12, 4.69829385735318e-12], [7.536696845713243e-12, 2.2063062163063806e-12], [5.118567840550299e-14, 7.006652139653629e-13], [1.0263530210194024e-12, 3.5962490414009e-15], [1.2126934660988946e-12, 4.674309675402737e-15], [6.38769701195559e-13, 1.9610809680592603e-15], [8.71064764357784e-12, 1.1473395054275779e-12], [3.445193997894766e-13, 2.417998548756978e-12], [8.421613397178105e-17, 1.3430010721469339e-14], [1.444176804611906e-13, 9.457103300464013e-13], [2.531027012895655e-12, 1.1099444568469858e-14], [3.2093264859766787e-12, 1.1012628769721437e-12], [4.8479087327608e-14, 3.1706040338977582e-12], [1.4581482430804865e-13, 4.2621920996543755e-12], [8.242975164202811e-12, 2.1301454434473086e-12], [4.2933450564589405e-12, 2.3844382268115373e-12], [5.480605550978512e-12, 1.0194788101998096e-13], [4.316422944139501e-12, 7.867267927458409e-13], [2.190480060764862e-11, 4.400711054716082e-12], [2.4497759133570623e-12, 1.199530073342211e-12], [3.6184278724732035e-12, 1.2874970823818735e-14], [1.4096393746557144e-11, 7.591662178454526e-13], [7.0261171700648124e-15, 9.905783110423539e-14], [9.433223405355429e-12, 9.000300656885228e-13], [9.088718923635261e-12, 4.887068316144974e-12], [2.3147260733119392e-11, 6.5721045348715585e-12], [5.325688162469493e-13, 2.1220916110830018e-13], [6.00682891787961e-12, 5.812109812512902e-13], [2.5319490123479407e-12, 2.195228839436296e-12], [2.0301842068157547e-12, 4.445007712706734e-12], [1.8459299107702258e-13, 6.144439298544734e-13], [1.693814184505874e-12, 4.512675007371157e-12], [1.249614082483721e-11, 1.4713541361146408e-11], [2.309557813178202e-12, 1.0366341175325033e-13], [1.2311033398177074e-14, 7.321852169232321e-13], [2.902054872304953e-20, 3.623589232586715e-15], [4.298722673556024e-12, 5.283832919476736e-13], [1.2485381033284345e-11, 8.362609746584603e-13], [3.026546348952183e-14, 4.75087533127598e-13], [2.8106358746385446e-12, 6.3867112288667865e-12], [5.083920972266802e-12, 2.7254048371580348e-12], [3.5307539905146655e-13, 6.054743552164446e-16]]\n",
      "\n",
      " ***  Fold 3 Performance:***************\n",
      "Accuracy: 80.43478260869566 Precision: 77.77777777777779 Recall: 73.68421052631578 F1-Score: 75.67567567567566 \n",
      "\n",
      "[['132' '6.20' '6.47' ... '14.14' '45' '0']\n",
      " ['123' '0.05' '4.61' ... '2.78' '16' '0']\n",
      " ['128' '0.50' '3.70' ... '22.73' '28' '0']\n",
      " ...\n",
      " ['138' '4.50' '2.85' ... '24.89' '56' '1']\n",
      " ['170' '7.60' '5.50' ... '6.17' '54' '1']\n",
      " ['128' '0.00' '10.58' ... '14.66' '48' '0']]\n",
      "[[4.437420024280852e-19, 1.4325871662998454e-18], [1.5741689206627151e-13, 2.0778320237199616e-16], [5.133486910578502e-12, 1.0331620814810271e-12], [1.7998633855451586e-11, 4.2083371002701736e-12], [1.8890806467164073e-11, 1.0358136160519651e-12], [1.2553493348249356e-12, 1.83675651024294e-15], [4.0850378063405426e-14, 1.417035403744949e-12], [5.703346195614025e-13, 1.27288358396812e-15], [6.375125376849033e-13, 9.275007436692148e-16], [1.764497738697239e-15, 5.742682160590489e-14], [2.7497751094303718e-14, 2.326686314971595e-13], [1.911997215911876e-15, 5.4132670925180845e-14], [3.778775998307832e-12, 8.82122241998754e-12], [2.4782830141268518e-11, 3.3585300201916644e-12], [2.358620867537254e-13, 6.5829426966250495e-12], [1.0387886770382203e-11, 1.6119579682587902e-12], [4.043566343143263e-17, 1.5071548032654223e-14], [1.2466087195685273e-12, 9.563830996240349e-13], [4.536538122194945e-15, 2.164378793319368e-13], [8.642897492697207e-15, 6.689792773603987e-13], [1.1867002882587633e-12, 1.0834758628195698e-12], [9.414911774740891e-12, 3.651441174978977e-13], [2.5974192637438907e-16, 3.1808693117338438e-15], [2.0997021942310135e-12, 3.169576535208108e-12], [9.0572951690873e-13, 1.90942011110427e-13], [1.629210809605945e-11, 5.653277628929118e-12], [1.1803499135886287e-12, 2.3602500577205102e-12], [5.3258843544249854e-14, 3.1913990177918506e-12], [2.724954636898967e-11, 2.270859335809642e-12], [1.240377596553993e-12, 6.80728337404426e-13], [3.635308388748776e-14, 3.621870241160771e-12], [1.3640886556401994e-15, 3.1497980325268386e-13], [1.9648967832676855e-13, 1.708781656531948e-15], [8.978702672838848e-13, 1.198793819487572e-12], [8.475597298720163e-20, 2.004967394241352e-19], [1.2196499667601223e-12, 5.0408855562111695e-15], [5.9692413005930534e-12, 1.7643616072303598e-12], [3.946581184406738e-12, 4.70806736467573e-12], [2.3967476359141453e-13, 1.6465472657837498e-12], [1.4329998223230084e-11, 2.5854822376292345e-13], [4.952154291935652e-12, 1.1553671782549647e-11], [1.29747279697646e-11, 3.188322040823833e-12], [1.980885996221952e-11, 9.433898843053182e-12], [7.655632368078186e-12, 6.822386673182257e-13], [7.023843078581327e-12, 2.599967995551462e-12], [4.438027197476149e-18, 2.1277793782111945e-13]]\n",
      "\n",
      " ***  Fold 4 Performance:***************\n",
      "Accuracy: 71.73913043478261 Precision: 55.00000000000001 Recall: 73.33333333333333 F1-Score: 62.857142857142854 \n",
      "\n",
      "[['132' '6.20' '6.47' ... '14.14' '45' '0']\n",
      " ['123' '0.05' '4.61' ... '2.78' '16' '0']\n",
      " ['128' '0.50' '3.70' ... '22.73' '28' '0']\n",
      " ...\n",
      " ['138' '4.50' '2.85' ... '24.89' '56' '1']\n",
      " ['170' '7.60' '5.50' ... '6.17' '54' '1']\n",
      " ['128' '0.00' '10.58' ... '14.66' '48' '0']]\n",
      "[[1.2139776251216989e-11, 3.958900959090207e-12], [8.93613508839431e-12, 3.119317699369214e-13], [1.893735623117844e-12, 5.931594838203737e-13], [1.1105704987672155e-13, 1.0354316748734817e-12], [1.2942880371383734e-12, 5.504163205089969e-15], [2.2173528369215787e-12, 1.7318696779209349e-12], [3.6990246592516236e-14, 8.767136410469401e-14], [6.3163869966380486e-12, 4.878275430361524e-12], [7.953986091963734e-16, 8.914338704583948e-14], [5.890335331012884e-14, 2.3503158002642667e-13], [2.112819923865789e-12, 1.395860938299138e-12], [2.1265866222987273e-13, 3.830014126035825e-14], [2.494589119707571e-15, 1.3847993211895446e-13], [2.0815184583999404e-14, 2.241894142044966e-16], [9.829704169165086e-13, 1.2512073461366602e-12], [1.1591378930299526e-11, 5.985207526213853e-13], [1.2053245131206606e-16, 3.336884180137217e-14], [7.069677134347663e-14, 2.2680798915543383e-16], [2.3493830962377337e-11, 5.544710063232548e-12], [1.6279221501332488e-13, 1.9264893823992907e-16], [1.2473263999406266e-13, 6.907217207349066e-16], [4.139481925047161e-15, 1.0566244612323925e-12], [1.2922179227742906e-13, 2.812947212658346e-14], [1.915772519939146e-11, 3.3570607963986643e-12], [5.941493366106282e-12, 2.9768044558443717e-12], [1.7806296194856403e-13, 4.5051542612006133e-13], [7.993829369412585e-13, 1.164803371039226e-12], [1.0926816701024432e-16, 5.673838392080224e-14], [4.752495459600276e-12, 4.0021516855812964e-12], [9.045019769411575e-12, 4.308296387086333e-12], [1.0915470737236396e-12, 6.881117372749454e-14], [5.9225724433907026e-12, 7.910640071194075e-14], [2.0888750950517524e-11, 2.5536168057404367e-12], [6.293204349422703e-12, 2.352812276374363e-14], [1.768116882203097e-15, 3.9614594263997907e-13], [2.0688463423847086e-12, 5.978858944496973e-13], [5.956898803032356e-14, 7.117766928149743e-14], [2.5652475022724135e-14, 1.0910407927001058e-13], [2.9754132325789127e-12, 8.43542424520674e-14], [2.567464729569897e-13, 4.253022166773387e-13], [1.7862221708458494e-11, 2.08125437718101e-12], [8.176371380130695e-13, 8.932285582941155e-13], [2.695503094126271e-13, 7.725714732980711e-13], [6.464073467487345e-12, 6.266518788243451e-13], [1.105983414855484e-12, 6.633338973054317e-14], [4.384937959180193e-13, 3.666744851051319e-12]]\n",
      "\n",
      " ***  Fold 5 Performance:***************\n",
      "Accuracy: 69.56521739130434 Precision: 70.58823529411765 Recall: 57.14285714285714 F1-Score: 63.15789473684211 \n",
      "\n",
      "[['132' '6.20' '6.47' ... '14.14' '45' '0']\n",
      " ['123' '0.05' '4.61' ... '2.78' '16' '0']\n",
      " ['128' '0.50' '3.70' ... '22.73' '28' '0']\n",
      " ...\n",
      " ['138' '4.50' '2.85' ... '24.89' '56' '1']\n",
      " ['170' '7.60' '5.50' ... '6.17' '54' '1']\n",
      " ['128' '0.00' '10.58' ... '14.66' '48' '0']]\n",
      "[[9.320273587430576e-15, 8.444188897400385e-14], [1.9410647117915917e-11, 1.914235503405588e-12], [1.997889143871578e-13, 1.1055490195655026e-13], [1.2658380794824903e-11, 2.659555628274342e-12], [6.664212665686305e-13, 1.17274150424531e-12], [6.377541882440017e-13, 2.9242447685049003e-12], [2.7586333738163252e-11, 3.348687811175714e-12], [2.430819589624417e-13, 3.9780056024508364e-13], [8.755805323015876e-17, 1.2931345190638751e-14], [4.954940613212239e-13, 5.451871128526878e-13], [5.52863349264725e-13, 1.1686890693374743e-12], [4.014941909676558e-12, 4.253074584117327e-12], [1.579154715598454e-11, 8.4652310127289e-12], [2.303764582835818e-12, 5.534928349718618e-12], [1.666534052505808e-11, 3.727655180643352e-12], [2.340752917351261e-12, 5.3330048518806125e-12], [2.1488062315331882e-12, 9.21809045638967e-12], [8.676948263458625e-12, 3.6875741445821945e-13], [9.068979775401069e-14, 8.232394408946945e-13], [1.233148685419753e-11, 1.546245617065582e-11], [3.4613846917908773e-12, 4.570990965037395e-13], [1.9838837824895915e-13, 7.814986477702173e-13], [4.918383419541741e-12, 9.907139975478756e-13], [2.0818920451725358e-11, 1.8986318213816363e-12], [5.227651637799848e-13, 1.0201800368096023e-12], [8.129847389751189e-13, 1.8506895783947325e-15], [7.833485163812084e-13, 5.529075826657892e-13], [8.817575440659895e-15, 7.679229514038095e-15], [7.406301986084021e-13, 9.139066630022733e-13], [1.1376098728642047e-14, 1.1039255377226055e-12], [1.6427243348179875e-14, 4.455833763781907e-13], [3.486428843302204e-12, 2.161569394200416e-13], [3.8818846160886486e-13, 9.870670100501095e-16], [8.918898155841355e-16, 6.667428953630493e-14], [4.997696665229973e-14, 1.600295024528135e-12], [6.73234718535279e-12, 6.065079461271141e-13], [1.1517205549552352e-11, 6.401257730291709e-14], [8.003294086335827e-14, 3.3552998920218835e-17], [4.0751414409873137e-13, 2.789718686794293e-13], [7.168352048663286e-13, 6.149765528932102e-12], [9.96940605473187e-12, 6.3328196230865825e-12], [9.90133881348558e-14, 8.312823072082335e-13], [1.885274597581554e-11, 3.773050556834831e-13], [8.038856644868275e-21, 8.349846217274153e-16], [7.839016172251814e-13, 2.1640642002920763e-12], [1.426314137036549e-11, 4.667115176734199e-12]]\n",
      "\n",
      " ***  Fold 6 Performance:***************\n",
      "Accuracy: 58.69565217391305 Precision: 48.0 Recall: 66.66666666666666 F1-Score: 55.81395348837209 \n",
      "\n",
      "[['132' '6.20' '6.47' ... '14.14' '45' '0']\n",
      " ['123' '0.05' '4.61' ... '2.78' '16' '0']\n",
      " ['128' '0.50' '3.70' ... '22.73' '28' '0']\n",
      " ...\n",
      " ['138' '4.50' '2.85' ... '24.89' '56' '1']\n",
      " ['170' '7.60' '5.50' ... '6.17' '54' '1']\n",
      " ['128' '0.00' '10.58' ... '14.66' '48' '0']]\n",
      "[[2.646326526666696e-12, 4.747902638547295e-12], [1.2826422021835998e-11, 3.1391104753692326e-12], [5.4693679809490646e-12, 9.570399656769412e-12], [2.1301452568848816e-11, 5.691320521120674e-12], [9.96207033149686e-13, 4.93851327061762e-12], [1.2583172583988083e-12, 4.907491390525664e-13], [1.710773206363513e-11, 3.3852929242083757e-12], [1.41324440025882e-12, 5.53364062158181e-12], [1.7704205433279905e-13, 2.2091256660198363e-12], [3.890830067015155e-13, 1.6203906788519596e-16], [6.605108974656937e-13, 7.904981982965784e-12], [1.012475943989376e-11, 2.2496332455456095e-13], [1.2878519341149541e-12, 9.53962844762122e-16], [6.1953721611213215e-12, 3.0906664646377406e-12], [5.672987290419477e-13, 6.029445642525169e-16], [1.482673926211329e-19, 1.6308813824240964e-16], [2.1874452936915386e-14, 5.449967528773412e-17], [1.195984436351489e-13, 2.0964557825117629e-13], [9.822423583616943e-15, 4.5154037769824955e-13], [5.044029462845464e-16, 7.009466527116952e-16], [5.304456866341354e-13, 6.754657218584732e-16], [2.0646702119761556e-12, 4.8924138560190116e-12], [9.636007165490958e-12, 2.5289832150853722e-12], [3.0896036940189323e-13, 4.998072776874143e-13], [1.6077579609493106e-11, 1.5695741665764667e-11], [2.893297123640803e-14, 1.086144185251189e-12], [7.85691648376332e-12, 2.1085403840267853e-13], [2.8787790257934493e-25, 1.202941485547137e-17], [1.8672142588373625e-11, 1.0066375438422688e-11], [1.100178222628731e-11, 1.1305886717499807e-11], [9.022169128036292e-13, 4.957441457802925e-16], [8.971285728983305e-12, 2.5767577964920033e-12], [5.331254860985243e-12, 5.136634181883048e-13], [7.61917520180617e-16, 2.2240483998336472e-14], [5.253011458039295e-12, 4.58710316215135e-13], [2.225322153459837e-13, 2.7887775489201184e-15], [7.827826042819175e-12, 1.9983840130162073e-12], [5.103955558067056e-13, 6.502042321321085e-13], [8.817485131000119e-19, 1.8277543269115552e-20], [1.7698388803699897e-13, 1.2642100585341484e-13], [2.2207683644269523e-12, 1.235862141040845e-13], [2.8242895468726746e-11, 1.2650528587526418e-11], [1.8137292546624836e-13, 3.357500575273042e-14], [8.839244836610005e-12, 7.487410743001578e-12], [1.5350248901014114e-12, 4.0632475778396156e-14], [1.2178711439388395e-12, 7.464737057486059e-16]]\n",
      "\n",
      " ***  Fold 7 Performance:***************\n",
      "Accuracy: 76.08695652173914 Precision: 56.25 Recall: 69.23076923076923 F1-Score: 62.06896551724138 \n",
      "\n",
      "[['132' '6.20' '6.47' ... '14.14' '45' '0']\n",
      " ['123' '0.05' '4.61' ... '2.78' '16' '0']\n",
      " ['128' '0.50' '3.70' ... '22.73' '28' '0']\n",
      " ...\n",
      " ['138' '4.50' '2.85' ... '24.89' '56' '1']\n",
      " ['170' '7.60' '5.50' ... '6.17' '54' '1']\n",
      " ['128' '0.00' '10.58' ... '14.66' '48' '0']]\n",
      "[[8.081344514036455e-12, 1.1422286297006386e-13], [6.245462356623971e-13, 4.2587032385931156e-16], [7.0952633752137415e-12, 3.633708773970492e-12], [8.500192850491298e-16, 6.580050838182406e-14], [6.414169980876365e-14, 6.491556671670166e-13], [1.1802467278222137e-11, 8.442176945207063e-13], [3.0910324593858775e-13, 6.8393061013898185e-16], [5.702534883896437e-14, 2.255706612138247e-16], [1.436777155383342e-11, 1.454805179854021e-12], [2.1545480970997453e-11, 5.2880515646107924e-12], [2.2645068463810483e-13, 2.7791750122316542e-14], [4.825085739136956e-13, 1.9146868495286363e-12], [1.0829764323727964e-12, 1.8071340965405604e-15], [1.0745058163866418e-11, 3.76447463988606e-12], [1.8166856023836807e-11, 1.6974887084771448e-12], [2.411858123588755e-13, 1.0532727517477663e-15], [8.753509642360912e-12, 4.662283658311891e-13], [3.2782384165121636e-15, 1.7977764043095248e-13], [2.320407871841555e-12, 1.0406674300380188e-12], [1.0169397430489225e-11, 1.9162946202658183e-12], [5.286091267319104e-13, 9.298056058357588e-13], [5.087650534643464e-12, 1.0717519410289559e-11], [5.336474461852125e-12, 3.0472389335242824e-12], [3.1229698413059776e-14, 2.626487704220902e-13], [2.15388903576787e-12, 4.084743124650174e-15], [1.8325698298787077e-13, 5.654477897100564e-13], [1.1837623801516241e-11, 2.977564830973071e-12], [6.87469892565152e-13, 4.186390826201991e-14], [6.727200680847196e-12, 9.897299407443896e-14], [8.044673814398354e-12, 3.048793776415701e-12], [7.785226668569428e-13, 4.6458712483804436e-12], [7.599548158163554e-13, 9.909080813299843e-16], [1.8105037315920826e-13, 2.278815158810815e-12], [8.589283326437903e-12, 1.9062502585509596e-13], [3.73196084263276e-12, 3.714856734747491e-12], [3.6556875727149155e-12, 1.0472614691345593e-14], [3.127841235197217e-19, 3.852750146565627e-17], [2.3326695970949e-13, 7.106191802293725e-13], [8.089870589439159e-14, 6.154905823857505e-14], [5.238955196546991e-12, 4.676338751944332e-12], [3.674355218755319e-12, 1.5999112133865228e-12], [4.30410242033137e-12, 8.878622102268285e-14], [1.220812608469077e-13, 2.3463512426261682e-12], [6.374486160741662e-12, 1.680689567337292e-14], [3.668776917195789e-15, 8.884361613748637e-14], [1.1317295489513194e-13, 1.899063784406012e-15]]\n",
      "\n",
      " ***  Fold 8 Performance:***************\n",
      "Accuracy: 73.91304347826086 Precision: 43.75 Recall: 70.0 F1-Score: 53.84615384615385 \n",
      "\n",
      "[['132' '6.20' '6.47' ... '14.14' '45' '0']\n",
      " ['123' '0.05' '4.61' ... '2.78' '16' '0']\n",
      " ['128' '0.50' '3.70' ... '22.73' '28' '0']\n",
      " ...\n",
      " ['138' '4.50' '2.85' ... '24.89' '56' '1']\n",
      " ['170' '7.60' '5.50' ... '6.17' '54' '1']\n",
      " ['128' '0.00' '10.58' ... '14.66' '48' '0']]\n",
      "[[2.5431615790323487e-12, 6.3152181018031515e-12], [3.123923731174948e-11, 6.4535142202966264e-12], [7.43771509177028e-12, 6.288936872164535e-12], [1.0961570814979172e-13, 1.088124225231783e-12], [2.03765797018113e-14, 7.475270292212443e-13], [2.233031728783415e-16, 2.9564102915380465e-13], [4.309629799518046e-12, 7.82474759160789e-15], [1.4290500213399445e-13, 4.883618110899281e-12], [8.690332067304816e-12, 2.1568875701244793e-13], [9.381731778496144e-14, 1.551728904965743e-12], [1.0282245725910609e-12, 5.101279211307841e-12], [2.442820952183251e-13, 8.153018047536298e-17], [2.5422486486009302e-12, 8.635248844974246e-13], [3.3030347387127714e-12, 3.1485979901704396e-15], [3.206276098838953e-12, 3.1206508471020557e-15], [1.509841517963284e-11, 1.020279614574755e-12], [1.775721891651529e-13, 7.43683196184874e-13], [1.0079621758205096e-12, 3.8080283540704735e-12], [1.5967582178218467e-17, 1.1829261959330454e-13], [9.073270744975902e-15, 6.427092469159069e-13], [1.551070388243303e-13, 1.1740129509599316e-12], [6.350858604189558e-12, 3.0422783406753106e-12], [4.2382842959340754e-23, 1.2370679519934357e-15], [5.001773899987444e-12, 3.6188349214652026e-12], [4.714232563634367e-12, 1.9626101740455847e-13], [2.302117301656839e-12, 2.179347166337515e-15], [6.977396313709801e-13, 6.176699834453504e-16], [1.6180780263264535e-11, 5.056549540939038e-13], [3.200018844875242e-12, 3.572218863490363e-14], [6.578597164155699e-12, 6.8432086314513624e-12], [1.0295172617502502e-11, 5.019394519503501e-13], [1.1417754692029913e-12, 5.690675608814415e-16], [5.842070144346635e-13, 2.973786415331561e-12], [2.409656247724463e-11, 5.147236854450476e-12], [6.1946431880880586e-15, 1.0632916126584402e-12], [3.549328409734038e-12, 7.724031264988858e-14], [4.024786432035245e-13, 2.1448594766818794e-12], [1.4363093253576286e-12, 1.397369934606419e-13], [1.2255705243691866e-12, 1.2244954208210697e-14], [9.363298769026605e-14, 1.190351442417259e-12], [2.250952466968564e-12, 4.349877545003854e-15], [8.680793732434227e-12, 2.037352127817863e-12], [2.2334158943304944e-12, 9.710722597127413e-13], [3.549687861729189e-16, 2.918661257129562e-14], [2.8422505296198904e-12, 8.582768886438917e-12], [1.521583456822361e-12, 1.9045857839807453e-13]]\n",
      "\n",
      " ***  Fold 9 Performance:***************\n",
      "Accuracy: 58.69565217391305 Precision: 42.857142857142854 Recall: 56.25 F1-Score: 48.64864864864864 \n",
      "\n",
      "[['132' '6.20' '6.47' ... '14.14' '45' '0']\n",
      " ['123' '0.05' '4.61' ... '2.78' '16' '0']\n",
      " ['128' '0.50' '3.70' ... '22.73' '28' '0']\n",
      " ...\n",
      " ['118' '6.00' '9.65' ... '0.00' '48' '0']\n",
      " ['136' '7.50' '7.39' ... '0.00' '45' '1']\n",
      " ['108' '0.80' '2.47' ... '0.00' '55' '1']]\n",
      "[[5.954692054502546e-12, 1.2530137416505813e-12], [2.2190502124256544e-12, 4.394734709042066e-12], [1.0921407404247144e-11, 1.2412821804646962e-12], [1.3697506483061713e-11, 8.825772160980372e-13], [8.364324910110585e-13, 1.058617463202789e-15], [8.788368382999605e-14, 1.5435578361558587e-12], [7.6759922164415e-12, 4.4304040018931195e-13], [1.865696417067821e-11, 4.75363832004803e-12], [3.280776408000398e-12, 3.014973908602253e-12], [4.605998344445798e-13, 4.770687102647678e-15], [1.6884326660024354e-15, 1.01777765150825e-12], [1.2226206092398572e-11, 8.06252812054475e-13], [5.0243211460674485e-12, 1.2996369696322937e-12], [1.594670129866653e-11, 1.1059590514888965e-12], [1.4381518641128385e-11, 1.2797963245383047e-12], [1.4739632298984729e-12, 8.022954153622186e-15], [1.1211301841356282e-12, 2.9208102733758176e-12], [1.6395810772445987e-13, 2.71461671968859e-13], [3.3557941052912515e-13, 3.710791702444702e-16], [7.414318202175149e-12, 6.456249084245787e-12], [5.308013488855981e-13, 4.773679319452965e-13], [2.1870217251753626e-12, 9.514957031240179e-12], [2.0307229224634636e-11, 2.8728338907375165e-12], [2.4951077752942876e-18, 2.8534374587396885e-17], [4.48235526941532e-13, 2.5270808647377137e-12], [8.388178861848261e-15, 2.0601502534548152e-13], [2.219166233810923e-12, 1.4704683886629654e-12], [3.804061931242893e-12, 1.957091601815671e-12], [1.588559215202641e-12, 5.137875654776735e-12], [2.339052795858797e-13, 8.861316757296583e-13], [2.893064408461018e-14, 2.5063285354097203e-13], [1.6802639540903353e-13, 3.0016168012371485e-13], [3.311103319008282e-13, 4.952471093210651e-13], [5.686815803252619e-12, 1.9564697309181403e-12], [5.3284580853976314e-12, 3.963005155888262e-13], [8.127081847243731e-16, 1.538111833907361e-14], [2.0809174522273176e-12, 1.8928624831792155e-12], [7.1807860968292934e-12, 9.541798598266605e-13], [3.807380657003397e-12, 1.8499318230125954e-14], [7.509771427467901e-13, 2.8595119653534812e-12], [5.775672036101065e-15, 2.0236245797125644e-13], [7.052956195947024e-12, 5.592282371260096e-13], [1.1646773652320966e-15, 5.253238696150485e-15], [2.1346916047142507e-12, 5.609417466995441e-13], [9.524986928405114e-13, 4.214801199036308e-15], [1.0030649090195867e-11, 8.429774659947494e-12], [2.2795183585665115e-15, 5.170648202058806e-14], [4.00697349925978e-14, 4.593475889715684e-13]]\n",
      "\n",
      " ***  Fold 10 Performance:***************\n",
      "Accuracy: 66.66666666666666 Precision: 47.61904761904761 Recall: 66.66666666666666 F1-Score: 55.55555555555556 \n",
      "\n",
      "Mean accuracy is :  69.05797101449274\n",
      "Mean precision is :  54.60347501319369\n",
      "Mean recall is :  69.7260217852323\n",
      "Mean fmeasure is :  60.415460257053006\n"
     ]
    }
   ],
   "source": [
    "kfold_validation = 10\n",
    "initial_set = 0\n",
    "\n",
    "total_sets = len(input_data) // kfold_validation\n",
    "remaining_sets = len(input_data) % kfold_validation\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "fmeasure = []\n",
    "\n",
    "for i in range(0,kfold_validation):\n",
    "\n",
    "    testdata = input_data\n",
    "    trainingdata = input_data\n",
    "    test_labels = testdata[:,-1]\n",
    "    final_set = initial_set + total_sets\n",
    "\n",
    "    if i == kfold_validation-1:\n",
    "        final_set = final_set + remaining_sets\n",
    "\n",
    "    testdata = testdata[initial_set:final_set]\n",
    "    test_labels = test_labels[initial_set:final_set]\n",
    "    trainingdata = np.delete(trainingdata, np.s_[initial_set:final_set], axis=0)\n",
    "\n",
    "    predicted_labels = naivebayes(trainingdata, testdata)\n",
    "    actual = convert_labels(test_labels)\n",
    "    metrics_dict = calculate_performance_metrics(actual, predicted_labels)\n",
    "    \n",
    "    print(\"\\n ***  Fold \"+str(i+1)+\" Performance:***************\")\n",
    "    print('Accuracy:',metrics_dict['accuracy'], 'Precision:', metrics_dict['precision'],'Recall:', metrics_dict['recall'], 'F1-Score:', metrics_dict['F1'] ,'\\n')\n",
    "\n",
    "    recall.append(metrics_dict['recall'])\n",
    "    precision.append(metrics_dict['precision'])\n",
    "    fmeasure.append(metrics_dict['F1'])\n",
    "    accuracy.append(metrics_dict['accuracy'])\n",
    "\n",
    "    initial_set = final_set\n",
    "\n",
    "mean_accuracy = np.mean(accuracy)\n",
    "mean_precision = np.mean(precision)\n",
    "mean_recall = np.mean(recall)\n",
    "mean_fmeasure = np.mean(fmeasure)\n",
    "\n",
    "print(\"Mean accuracy is : \",mean_accuracy)\n",
    "print(\"Mean precision is : \",mean_precision)\n",
    "print(\"Mean recall is : \",mean_recall)\n",
    "print(\"Mean fmeasure is : \",mean_fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
