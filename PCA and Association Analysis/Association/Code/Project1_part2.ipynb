{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the library \n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "#from matplotlib import pyplot as plt\n",
    "import random\n",
    "import matplotlib\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFYING THE DATA INTO A MEANINGFUL FORMAT\n",
    "def modify_data(filename):\n",
    "    #reading the data from the csv file\n",
    "    file = open(filename, 'r')\n",
    "    lines_list = file.readlines()\n",
    "    #data = pd.read_csv(filename, sep=\"\\t\", header=None)\n",
    "    #retrieve the last col of the data ie the disease name\n",
    "    #label = data.iloc[:,-1]\n",
    "    #drop the last column from the data\n",
    "    #data = data.drop(data.columns[len(data.columns)-1], axis=1)\n",
    "    genes_data = []\n",
    "    for line in lines_list:\n",
    "        # split line and store as expression list\n",
    "        genes = line.replace(\"\\n\",\"\").split('\\t')\n",
    "        genes_transformed_list = []\n",
    "        i = 1\n",
    "        for gene in genes:\n",
    "            if i == len(genes): #Exclude the disease names\n",
    "                genes_transformed_list.append(gene)\n",
    "            else:\n",
    "                genes_transformed_list.append(\"G\" + str(i) + \"_\" + gene)\n",
    "            i += 1\n",
    "        genes_data.append(genes_transformed_list)\n",
    "        \n",
    "#     for i in range(len(data)):\n",
    "#         counter = 1\n",
    "#         for j in range(len(data[0])):\n",
    "#             if j == len(data):\n",
    "#                 data[i][j] = data[i][j]\n",
    "#             else:\n",
    "#                 data[i][j] = \"G\"+str(counter)+\"_\"+str(data[i][j])\n",
    "#                 counter = counter+1\n",
    "    return genes_data\n",
    "\n",
    "\n",
    "#obtain unique elements to find their frequency\n",
    "def obtain_unique(data):\n",
    "    unique_elements = set()\n",
    "    element_list = list()\n",
    "    element = list()\n",
    "    for i in data:\n",
    "        element.append(i)\n",
    "        element_list.append(frozenset(i)) \n",
    "    unique_element = frozenset(np.unique(element))\n",
    "    for j in unique_element:\n",
    "        unique_elements.add(frozenset([j]))\n",
    "    return element_list, unique_elements\n",
    "\n",
    "#check if the item set satifies the support\n",
    "def check_support(localDict,minSupport):\n",
    "    itemSet = set()\n",
    "    for item, count in localDict.items():\n",
    "                support = float(count)/len(element_list)\n",
    "                if support >= minSupport:\n",
    "                        itemSet.add(item)\n",
    "    return itemSet\n",
    "\n",
    "#checks if the elements exist in the original list\n",
    "def findMinimumSupportList(unique_elements,element_list,minSupport,local_list):\n",
    "    \n",
    "    listSize = len(element_list)\n",
    "    localDict = defaultdict(int)\n",
    "    itemSet = set()\n",
    "    \n",
    "    \n",
    "    for item in unique_elements:\n",
    "        for transaction in element_list:\n",
    "            if item.issubset(transaction):\n",
    "                local_list[item] += 1\n",
    "                localDict[item] += 1\n",
    "\n",
    "    itemSet = check_support(localDict,minSupport)\n",
    "    \n",
    "    return itemSet\n",
    "\n",
    "# create the set with the subset for each let that satisfies the support\n",
    "def joinSet(itemSet, length):\n",
    "        \"\"\"Join a set with itself and returns the n-element itemsets\"\"\"\n",
    "        join_set = set()\n",
    "        for i in itemSet:\n",
    "            for j in itemSet:\n",
    "                if len(i.union(j)) == length:\n",
    "                    join_set.add(i.union(j))\n",
    "                    \n",
    "        return join_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFrequentItemSets(element_list,unique_elements, support_percentage):\n",
    "    updated_list = dict()\n",
    "    local_list = defaultdict(int)\n",
    "    transactionCount = len(element_list)\n",
    "    minSupport = support_percentage/100\n",
    "    firstIteration = findMinimumSupportList(unique_elements,element_list,minSupport,local_list)\n",
    "    \n",
    "    lSet = firstIteration\n",
    "    k = 2\n",
    "        \n",
    "    while(lSet != set([])):\n",
    "        updated_list[k-1] = lSet\n",
    "        lSet = joinSet(lSet, k)\n",
    "        cSet = findMinimumSupportList(lSet,element_list,minSupport,local_list)\n",
    "        lSet = cSet\n",
    "        k = k + 1\n",
    "    return updated_list,local_list,transactionCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_frequent_itemset_dict(globalFreqList):\n",
    "    # print(globalFreqList)\n",
    "    total_list = []\n",
    "    updated_globalFreqList = dict.fromkeys(globalFreqList)\n",
    "    for key in globalFreqList:\n",
    "        updated_globalFreqList[key] = (list(globalFreqList[key]))\n",
    "    frequent_set_dict = dict()\n",
    "    for key in updated_globalFreqList:\n",
    "        sets = updated_globalFreqList[key]\n",
    "    #     print(sets)\n",
    "        new_set = []\n",
    "        for i in sets:\n",
    "            #print(list(i))\n",
    "            new_set.append(list(i))\n",
    "        #print(new_set)\n",
    "        total_list.append(new_set)\n",
    "        frequent_set_dict.update(key=new_set)\n",
    "    #print(total_list)\n",
    "    joined_list = []\n",
    "    for i in range(len(total_list)):\n",
    "        if i>=0:\n",
    "            temp = total_list[i]\n",
    "            for j in range(len(temp)):\n",
    "                joined_list.append(','.join(temp[j]))\n",
    "\n",
    "    dict_frequent_itemsets = defaultdict(list)\n",
    "    for itemset in joined_list:\n",
    "        #print(itemset)\n",
    "        length_of_itemset = len(itemset.split(\",\"))\n",
    "        dict_frequent_itemsets[length_of_itemset].append(itemset)\n",
    "#     print(dict_frequent_itemsets)\n",
    "    return dict_frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ['G33_Down', 'G38_Down', 'G72_Up', 'G30_Down', 'G45_Up', 'G6_Up', 'G64_Up', 'G10_Down', 'G32_Down', 'G52_Down', 'G75_Up', 'G2_Down', 'G35_Down', 'G85_Down', 'G65_Down', 'G23_Up', 'G88_Down', 'G84_Up', 'G63_Down', 'G14_Up', 'G51_Down', 'G61_Down', 'G25_Up', 'G3_Up', 'G20_Down', 'G56_Up', 'G100_Down', 'G26_Up', 'G49_Down', 'G4_Down', 'G68_Down', 'G57_Down', 'G87_Up', 'G48_Up', 'G37_Up', 'G69_Down', 'G73_Down', 'G5_Up', 'G83_Up', 'G15_Down', 'G90_Up', 'G24_Down', 'G66_Up', 'G91_Up', 'G63_Up', 'G17_Up', 'G34_Down', 'G71_Up', 'G94_Up', 'G21_Up', 'G22_Up', 'G43_Up', 'G53_Down', 'G41_Down', 'G31_Up', 'G92_Down', 'G28_Down', 'G42_Down', 'G16_Down', 'G97_Down', 'G98_Up', 'G81_Up', 'G54_Up', 'G53_Up', 'G50_Up', 'G12_Up', 'G74_Down', 'G47_Up', 'G9_Up', 'G18_Down', 'G55_Up', 'G82_Down', 'G44_Down', 'G3_Down', 'G29_Down', 'G70_Down', 'G7_Down', 'G46_Down', 'G96_Down', 'G58_Down', 'G9_Down', 'G39_Up', 'G64_Down', 'G95_Down', 'G99_Up', 'G36_Up', 'G86_Down', 'G77_Up', 'G50_Down', 'G89_Up', 'G13_Down', 'G67_Up', 'G93_Up', 'G27_Up', 'G43_Down', 'G79_Down', 'G60_Up', 'G4_Up', 'G59_Up', 'G40_Down', 'G80_Down', 'G83_Down', 'G11_Down', 'G76_Down', 'G8_Up', 'G78_Up', 'G1_Up', 'G19_Down', 'G62_Down'], 2: ['G59_Up,G10_Down', 'G32_Down,G72_Up', 'G1_Up,G54_Up', 'G28_Down,G32_Down', 'G87_Up,G88_Down', 'G67_Up,G38_Down', 'G13_Down,G72_Up', 'G38_Down,G32_Down', 'G28_Down,G88_Down', 'G97_Down,G72_Up', 'G13_Down,G28_Down', 'G59_Up,G1_Up', 'G28_Down,G87_Up', 'G2_Down,G38_Down', 'G47_Up,G38_Down', 'G2_Down,G28_Down', 'G28_Down,G10_Down', 'G59_Up,G82_Down', 'G67_Up,G1_Up', 'G88_Down,G54_Up', 'G59_Up,G96_Down', 'G38_Down,G1_Up', 'G59_Up,G38_Down', 'G94_Up,G10_Down', 'G88_Down,G10_Down', 'G96_Down,G82_Down', 'G70_Down,G10_Down', 'G72_Up,G1_Up', 'G41_Down,G38_Down', 'G96_Down,G72_Up', 'G59_Up,G32_Down', 'G13_Down,G6_Up', 'G13_Down,G82_Down', 'G88_Down,G24_Down', 'G1_Up,G70_Down', 'G6_Up,G28_Down', 'G38_Down,G70_Down', 'G59_Up,G87_Up', 'G54_Up,G24_Down', 'G91_Up,G38_Down', 'G47_Up,G10_Down', 'G38_Down,G72_Up', 'G38_Down,G28_Down', 'G59_Up,G6_Up', 'G1_Up,G10_Down', 'G38_Down,G88_Down', 'G88_Down,G8_Up', 'G59_Up,G28_Down', 'G59_Up,G72_Up', 'G47_Up,G28_Down', 'G52_Down,G38_Down', 'G38_Down,G10_Down', 'G13_Down,G59_Up', 'G59_Up,G88_Down', 'G6_Up,G32_Down', 'G52_Down,G28_Down', 'G97_Down,G82_Down', 'G41_Down,G28_Down', 'G38_Down,G65_Down', 'G41_Down,G88_Down', 'G38_Down,G94_Up', 'G82_Down,G72_Up', 'G6_Up,G38_Down'], 3: ['G72_Up,G59_Up,G82_Down', 'G59_Up,G96_Down,G72_Up']}\n"
     ]
    }
   ],
   "source": [
    "dict_frequent_itemsets = dict(modify_frequent_itemset_dict(globalFreqList))\n",
    "print(dict_frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_itemset_rules(itemset_list, body_max_length, numerator, confidence_threshold_percentage, gene_matrix, rules_set):\n",
    "    itemset_list_set = set(itemset_list)\n",
    "\n",
    "    for body_length in range(body_max_length, 0, -1):\n",
    "        for body_list in list(itertools.combinations(itemset_list, body_length)):\n",
    "            body_set = set(body_list)\n",
    "            head_list = list(body_set ^ itemset_list_set)\n",
    "            rule = \",\".join(body_list) + \"->\" + \",\".join(head_list)\n",
    "            if rule not in rules_set:\n",
    "                denominator = 0\n",
    "                for gene_list in gene_matrix:\n",
    "                    gene_list_set = set(gene_list)\n",
    "                    if body_set < gene_list_set:\n",
    "                        denominator += 1\n",
    "                confidence_percentage = (numerator / denominator) * 100\n",
    "                if (confidence_percentage >= confidence_threshold_percentage):\n",
    "                    rules_set.add(rule)\n",
    "    return rules_set\n",
    "\n",
    "\n",
    "def generate_rules(rules_set, frequent_itemsets_dict, confidence_threshold_percentage, gene_matrix):\n",
    "    frequent_itemsets_keylist = list(frequent_itemsets_dict.keys())\n",
    "    frequent_itemsets_keylist.sort(reverse=True)\n",
    "\n",
    "    for itemset_length in frequent_itemsets_keylist:\n",
    "        # exclude itemsets of length 1 for rule generation\n",
    "        if itemset_length == 1:\n",
    "            break\n",
    "        else:\n",
    "            itemset_list = frequent_itemsets_dict[itemset_length]\n",
    "            for itemset_string in itemset_list:\n",
    "                itemset_list_set = set(itemset_string.split(\",\"))\n",
    "                if itemset_list_set not in rules_set:\n",
    "                    body_max_length = itemset_length - 1\n",
    "                    numerator = 0\n",
    "                    for gene_list in gene_matrix:\n",
    "                        gene_list_set = set(gene_list)\n",
    "                        if itemset_list_set < gene_list_set:\n",
    "                            numerator += 1\n",
    "                    if numerator > 0:\n",
    "                        itemset_string_list = itemset_string.split(\",\")\n",
    "                        #print(itemset_string_list)\n",
    "                        get_itemset_rules(itemset_string_list, body_max_length, numerator,\n",
    "                                          confidence_threshold_percentage, gene_matrix, rules_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_template1_input():\n",
    "    template1_query = input('Enter template-1 query (RULE|BODY|HEAD;ANY|NUMBER|NONE;ITEM1,ITEM2,...): ').split(';')\n",
    "    template1_query_part = template1_query[0]\n",
    "    template1_constraint = template1_query[1]\n",
    "    template1_genelist = template1_query[2].split(',')\n",
    "    return template1_query_part, template1_constraint, template1_genelist\n",
    "\n",
    "def get_overall_Set(rule, template_query_part, superset):\n",
    "    rule_list = rule.split('->')\n",
    "    head_list = rule_list[0].split(',')\n",
    "    body_list = rule_list[1].split(',')\n",
    "    if template_query_part == 'RULE':\n",
    "        superset = set(body_list) | set(head_list)\n",
    "    elif template_query_part == 'BODY':\n",
    "        superset = set(body_list)\n",
    "    elif template_query_part == 'HEAD':\n",
    "        superset = set(head_list)\n",
    "    return superset\n",
    "\n",
    "def get_template1_rules(rules_set, filtered_rules_set, template3_op):\n",
    "    template1_query_part, template1_constraint, template1_genelist = parse_template1_input()\n",
    "    template1_query_part = template1_query_part.upper()\n",
    "    template1_constraint = template1_constraint.upper()\n",
    "    superset = set()\n",
    "    for rule in rules_set:\n",
    "        superset = get_overall_Set(rule, template1_query_part, superset)\n",
    "        if template1_constraint == 'ANY' and len(set(template1_genelist) & superset) > 0:\n",
    "            filtered_rules_set.add(rule)\n",
    "        elif template1_constraint == 'NONE' and len(set(template1_genelist) & superset) == 0:\n",
    "            filtered_rules_set.add(rule)\n",
    "        elif template1_constraint == '1' and len(set(template1_genelist) & superset) == 1:\n",
    "            filtered_rules_set.add(rule)\n",
    "        elif template3_op == 'AND' and rule in filtered_rules_set:\n",
    "            filtered_rules_set.remove(rule)\n",
    "    #print(filtered_rules_set)\n",
    "    \n",
    "def parse_template2_input():\n",
    "    template2_query = input('Enter template-2 query (RULE|BODY|HEAD;SIZE): ').split(';')\n",
    "    template2_query_part = template2_query[0]\n",
    "    template2_size = int(template2_query[1])\n",
    "    return template2_query_part, template2_size\n",
    "\n",
    "def get_template2_rules(rules_set, filtered_rules_set, template3_op):\n",
    "    template2_query_part, template2_size = parse_template2_input()\n",
    "    template2_query_part = template2_query_part.upper()\n",
    "    superset = set()\n",
    "    for rule in rules_set:\n",
    "        superset = get_overall_Set(rule, template2_query_part, superset)\n",
    "        if len(superset) >= template2_size:\n",
    "            filtered_rules_set.add(rule)\n",
    "        elif template3_op == 'AND' and rule in filtered_rules_set:\n",
    "            filtered_rules_set.remove(rule)\n",
    "\n",
    "\n",
    "def parse_template3_input():\n",
    "    template3 = input('Enter template combination: ')\n",
    "    template3 = template3.upper()\n",
    "    if 'OR' in template3:\n",
    "        template3_first = template3.split('OR')[0]\n",
    "        template3_second = template3.split('OR')[1]\n",
    "        template3_op = 'OR'\n",
    "    elif 'AND' in template3:\n",
    "        template3_first = template3.split('AND')[0]\n",
    "        template3_second = template3.split('AND')[1]\n",
    "        template3_op = 'AND'\n",
    "    \n",
    "    return template3_first, template3_second, template3_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "Enter Support percentage: 50\n",
      "1. Support is set to be 50 %\n",
      "Number of  1 length frequent itemsets : 109\n",
      "Number of  2 length frequent itemsets : 63\n",
      "Number of  3 length frequent itemsets : 2\n",
      "Number of all length frequent itemsets : 174\n"
     ]
    }
   ],
   "source": [
    "#step 1 : obtain the data to generate the frequent itemsets\n",
    "data = modify_data(\"associationruletestdata.txt\")\n",
    "#print(label) \n",
    "element_list , unique_elements = obtain_unique(data)\n",
    "print(len(unique_elements))\n",
    "\n",
    "minSupport = int(input(\"Enter Support percentage: \"))\n",
    "globalFreqList,freqSetDict,transactionCount = findFrequentItemSets(element_list,unique_elements, minSupport)\n",
    "#print(globalFreqList)\n",
    "print(\"1. Support is set to be\",minSupport,\"%\")\n",
    "total =0\n",
    "for k in  globalFreqList:\n",
    "    print(\"Number of \",k,\"length frequent itemsets :\",len(globalFreqList[k]))\n",
    "    total = total + len(globalFreqList[k])\n",
    "print(\"Number of all length frequent itemsets :\", total)\n",
    "#     print(\"for k :\",k,\"count is :\",globalFreqList[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Support Percentage: 50\n",
      "Enter the Confidence Percentage: 70\n",
      "Number of rules for support 50% and confidence 70% : 117\n",
      "Enter template type: 1\n",
      "Enter template-1 query (RULE|BODY|HEAD;ANY|NUMBER|NONE;ITEM1,ITEM2,...): RULE;ANY;G59_Up\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'overall_rules' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-42867596c864>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtemplate3_operator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtemplate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mget_template1_rules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrules_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_rules_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate3_operator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mtemplate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-61071b495c42>\u001b[0m in \u001b[0;36mget_template1_rules\u001b[1;34m(rules_set, filtered_rules_set, template3_op)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mtemplate1_constraint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemplate1_constraint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0msuperset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moverall_rules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0msuperset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_overall_Set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate1_query_part\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuperset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtemplate1_constraint\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ANY'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate1_genelist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0msuperset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'overall_rules' is not defined"
     ]
    }
   ],
   "source": [
    "support_threshold_percentage = int(input(\"Enter the Support Percentage: \"))\n",
    "confidence_threshold_percentage = int(input(\"Enter the Confidence Percentage: \"))\n",
    "rules_set = set()\n",
    "generate_rules(rules_set, dict_frequent_itemsets, confidence_threshold_percentage, element_list)\n",
    "#print(rules_set)\n",
    "print(\"Number of rules for support \" +str(support_threshold_percentage)+ \"% and confidence \" +str(confidence_threshold_percentage)+ \"% : \"+str(len(rules_set)))\n",
    "template = input('Enter template type: ')\n",
    "filtered_rules_set = set()\n",
    "template3_operator = ''\n",
    "if template == '1':\n",
    "    get_template1_rules(rules_set, filtered_rules_set, template3_operator)\n",
    "\n",
    "elif template == '2':\n",
    "    get_template2_rules(rules_set, filtered_rules_set, template3_operator)\n",
    "\n",
    "elif template == '3':\n",
    "    template3_first, template3_second, template3_operator = get_template3_input()\n",
    "    if template3_first == '1':\n",
    "        get_template1_rules(rules_set, filtered_rules_set, template3_operator)\n",
    "    elif template3_first == '2':\n",
    "        get_template2_rules(rules_set, filtered_rules_set, template3_operator)\n",
    "    if template3_operator == 'AND':\n",
    "        updated_rules_set = copy.deepcopy(filtered_rules_set)\n",
    "    elif template3_operator == 'OR':\n",
    "        updated_rules_set = copy.deepcopy(rules_set)\n",
    "    if template3_second == '1':\n",
    "        get_template1_rules(updated_rules_set, filtered_rules_set, template3_operator)\n",
    "    elif template3_second == '2':\n",
    "        get_template2_rules(updated_rules_set, filtered_rules_set, template3_operator)\n",
    "\n",
    "\n",
    "print(filtered_rules_set)\n",
    "print(\"Count of filtered rules: \" + str(len(filtered_rules_set)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
